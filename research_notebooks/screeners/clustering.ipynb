{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üì¶ Initialize Data Source and Load Cached Data\n",
    "# Set up CLOB data source and load pre-cached candle data for analysis\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress non-critical warnings for cleaner output\n",
    "\n",
    "# Initialize CLOB data source and load cached candle data\n",
    "from core.data_sources.clob import CLOBDataSource\n",
    "\n",
    "\n",
    "clob = CLOBDataSource()\n",
    "clob.load_candles_cache()\n",
    "\n",
    "print(\"‚úÖ CLOB data source initialized and cache loaded\")\n",
    "print(f\"üìä Available cached datasets: {len(clob.candles_cache)} candle sets\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öôÔ∏è Analysis Configuration\n",
    "Define timeframe for clustering analysis"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "INTERVAL = \"15m\"  # 15-minute timeframe for mid-frequency relationship analysis\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"  ‚Ä¢ Timeframe: {INTERVAL}\")\n",
    "print(f\"  ‚Ä¢ Analysis Type: Cointegration-based hierarchical clustering\")\n",
    "print(f\"  ‚Ä¢ Method: Statistical relationship identification\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# üßÆ Enhanced Cointegration Matrix Calculation with Normalized Prices\n# Calculate pairwise cointegration relationships for all trading pairs\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.stattools import coint\nfrom tqdm.notebook import tqdm\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Filter candles for specified interval\ncandles = [candle for candle in clob.candles_cache.values() if candle.interval == INTERVAL]\nprint(f\"üìä Processing {len(candles)} trading pairs for cointegration analysis...\")\n\n# ============================================\n# NORMALIZED PRICE SERIES PREPARATION\n# ============================================\n# IMPORTANT: Use normalized prices (cumprod of returns) for cointegration!\n# This makes different price levels comparable (BTC at $60k vs DOGE at $0.10)\npair_normalized_prices = {}  # For cointegration (normalized prices)\npair_returns = {}  # For correlation analysis\nmin_data_points = 100  # Minimum required data points\n\nfor candle in candles:\n    df = candle.data\n    if df is not None and not df.empty and len(df) >= min_data_points:\n        pair_name = candle.trading_pair\n        \n        # Calculate returns\n        returns = df['close'].pct_change()\n        \n        # Calculate normalized prices (cumulative product starting from 1)\n        # This gives us comparable price series regardless of absolute price level\n        normalized_prices = (1 + returns).cumprod()\n        normalized_prices.iloc[0] = 1  # Ensure first value is 1\n        \n        # Store both series\n        pair_normalized_prices[pair_name] = normalized_prices.dropna()\n        pair_returns[pair_name] = returns.dropna()\n\npairs = list(pair_normalized_prices.keys())\nn_pairs = len(pairs)\n\nprint(f\"‚úÖ Prepared normalized price series for {n_pairs} trading pairs\")\nprint(f\"üìä All price series start at 1.0 for fair comparison\")\nprint(f\"üìä Minimum data points per series: {min_data_points}\")\n\n# ============================================\n# COINTEGRATION & CORRELATION MATRICES\n# ============================================\n# Initialize matrices\ncointegration_matrix = pd.DataFrame(\n    np.ones((n_pairs, n_pairs)),  # Initialize with 1 (no cointegration)\n    index=pairs,\n    columns=pairs\n)\n\ncorrelation_matrix = pd.DataFrame(\n    np.zeros((n_pairs, n_pairs)),\n    index=pairs,\n    columns=pairs\n)\n\nprint(f\"üîÑ Calculating cointegration p-values and correlations...\")\n\n# Track analysis results\nanalysis_results = {\n    'successful_tests': 0,\n    'failed_tests': [],\n    'strong_cointegration': [],\n    'strong_correlation': []\n}\n\n# Calculate pairwise relationships\nfor i in tqdm(range(n_pairs), desc=\"Processing pairs\"):\n    for j in range(i, n_pairs):\n        pair1, pair2 = pairs[i], pairs[j]\n        \n        if i == j:\n            # Self-relationship\n            cointegration_matrix.iloc[i, j] = 0.0  # Perfect cointegration with self\n            correlation_matrix.iloc[i, j] = 1.0   # Perfect correlation with self\n        else:\n            # Get aligned normalized price series for cointegration\n            norm_prices1 = pair_normalized_prices[pair1]\n            norm_prices2 = pair_normalized_prices[pair2]\n            norm_prices1_aligned, norm_prices2_aligned = norm_prices1.align(norm_prices2, join='inner')\n            \n            # Get aligned return series for correlation\n            returns1 = pair_returns[pair1]\n            returns2 = pair_returns[pair2]\n            returns1_aligned, returns2_aligned = returns1.align(returns2, join='inner')\n            \n            if len(norm_prices1_aligned) >= min_data_points:\n                try:\n                    # Calculate correlation (using returns)\n                    if len(returns1_aligned) > 0:\n                        corr = returns1_aligned.corr(returns2_aligned)\n                        correlation_matrix.iloc[i, j] = corr\n                        correlation_matrix.iloc[j, i] = corr\n                        \n                        if abs(corr) > 0.7:\n                            analysis_results['strong_correlation'].append({\n                                'pair1': pair1,\n                                'pair2': pair2,\n                                'correlation': corr\n                            })\n                    \n                    # Perform Engle-Granger cointegration test (using normalized prices)\n                    test_stat, p_value, critical_values = coint(norm_prices1_aligned, norm_prices2_aligned)\n                    \n                    # Store cointegration results\n                    cointegration_matrix.iloc[i, j] = p_value\n                    cointegration_matrix.iloc[j, i] = p_value\n                    \n                    analysis_results['successful_tests'] += 1\n                    \n                    if p_value < 0.05:\n                        analysis_results['strong_cointegration'].append({\n                            'pair1': pair1,\n                            'pair2': pair2,\n                            'p_value': p_value,\n                            'correlation': corr if 'corr' in locals() else np.nan\n                        })\n                    \n                except Exception as e:\n                    analysis_results['failed_tests'].append((pair1, pair2, str(e)))\n                    cointegration_matrix.iloc[i, j] = 1.0\n                    cointegration_matrix.iloc[j, i] = 1.0\n\nprint(\"‚úÖ Cointegration matrix calculation complete\\n\")\n\n# ============================================\n# ANALYSIS SUMMARY\n# ============================================\nprint(\"üìä Cointegration Analysis Results (using normalized prices):\")\nprint(\"(Lower p-values indicate stronger cointegration relationships)\")\n\n# Calculate relationship counts\nstrong_coint = ((cointegration_matrix < 0.05) & (cointegration_matrix > 0)).sum().sum() // 2\nmoderate_coint = ((cointegration_matrix >= 0.05) & (cointegration_matrix < 0.1)).sum().sum() // 2\nweak_coint = ((cointegration_matrix >= 0.1)).sum().sum() // 2\n\nprint(f\"  ‚Ä¢ Strong cointegration (p < 0.05): {strong_coint}\")\nprint(f\"  ‚Ä¢ Moderate cointegration (0.05 ‚â§ p < 0.1): {moderate_coint}\")\nprint(f\"  ‚Ä¢ Weak/No cointegration (p ‚â• 0.1): {weak_coint}\")\nprint(f\"  ‚Ä¢ Successful tests: {analysis_results['successful_tests']}\")\nprint(f\"  ‚Ä¢ Failed tests: {len(analysis_results['failed_tests'])}\")\n\n# Show correlation summary\nhigh_corr = ((correlation_matrix > 0.7) & (correlation_matrix < 1)).sum().sum() // 2\nmoderate_corr = ((correlation_matrix > 0.3) & (correlation_matrix <= 0.7)).sum().sum() // 2\nlow_corr = ((correlation_matrix <= 0.3) & (correlation_matrix > -1)).sum().sum() // 2\n\nprint(f\"\\nüìä Correlation Analysis Results:\")\nprint(f\"  ‚Ä¢ High correlation (>0.7): {high_corr}\")\nprint(f\"  ‚Ä¢ Moderate correlation (0.3-0.7): {moderate_corr}\")\nprint(f\"  ‚Ä¢ Low correlation (‚â§0.3): {low_corr}\")\n\n# Show top cointegrated pairs\nif analysis_results['strong_cointegration']:\n    print(f\"\\nüèÜ Top 5 Cointegrated Pairs (best for pairs trading):\")\n    sorted_coint = sorted(analysis_results['strong_cointegration'], key=lambda x: x['p_value'])\n    for i, pair_info in enumerate(sorted_coint[:5]):\n        print(f\"  {i+1}. {pair_info['pair1']} ‚Üî {pair_info['pair2']}: p={pair_info['p_value']:.4f}, corr={pair_info['correlation']:.3f}\")\n\n# ============================================\n# CORRELATION VS COINTEGRATION VISUALIZATION\n# ============================================\n# Create comparison visualization\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('Correlation Matrix', 'Cointegration P-Values (capped at 0.1)'),\n    horizontal_spacing=0.12\n)\n\n# Correlation heatmap\nfig.add_trace(\n    go.Heatmap(\n        z=correlation_matrix.values,\n        x=pairs,\n        y=pairs,\n        colorscale='RdBu',\n        zmid=0,\n        zmin=-1,\n        zmax=1,\n        colorbar=dict(title=\"Corr\", x=0.45, len=0.9),\n        text=correlation_matrix.values.round(2),\n        texttemplate=\"%{text}\",\n        textfont={\"size\": 8},\n        showscale=True\n    ),\n    row=1, col=1\n)\n\n# Cointegration heatmap (capped for better visualization)\ncoint_viz = np.minimum(cointegration_matrix.values, 0.1)\nfig.add_trace(\n    go.Heatmap(\n        z=coint_viz,\n        x=pairs,\n        y=pairs,\n        colorscale='RdYlGn_r',\n        zmin=0,\n        zmax=0.1,\n        colorbar=dict(title=\"P-Val\", x=1.02, len=0.9),\n        text=cointegration_matrix.values.round(3),\n        texttemplate=\"%{text}\",\n        textfont={\"size\": 8},\n        showscale=True\n    ),\n    row=1, col=2\n)\n\nfig.update_layout(\n    title='üìä Correlation vs Cointegration Analysis<br><sub>Using normalized prices (cumprod of returns) for fair comparison</sub>',\n    width=1600,\n    height=700,\n    template=\"plotly_dark\"\n)\n\nfig.update_xaxes(tickangle=-45, tickfont=dict(size=9))\nfig.update_yaxes(tickfont=dict(size=9))\nfig.show()\n\n# Display the cointegration matrix\nprint(\"\\nüìä Cointegration Matrix (p-values):\")\ncointegration_matrix",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# üìä Correlation vs Cointegration Scatter Analysis\n# Analyze the relationship between correlation and cointegration\nimport plotly.express as px\n\n# Prepare comparison data\ncomparison_data = []\nfor i in range(n_pairs):\n    for j in range(i+1, n_pairs):\n        comparison_data.append({\n            'pair': f\"{pairs[i]}-{pairs[j]}\",\n            'pair1': pairs[i],\n            'pair2': pairs[j],\n            'correlation': abs(correlation_matrix.iloc[i, j]),\n            'cointegration_pvalue': cointegration_matrix.iloc[i, j]\n        })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\n# Create scatter plot\nfig = px.scatter(\n    comparison_df,\n    x='correlation',\n    y='cointegration_pvalue',\n    hover_data=['pair1', 'pair2'],\n    title='üîç Correlation vs Cointegration Analysis',\n    labels={\n        'correlation': 'Absolute Correlation',\n        'cointegration_pvalue': 'Cointegration P-Value'\n    },\n    color='cointegration_pvalue',\n    color_continuous_scale='RdYlGn_r'\n)\n\n# Add reference lines\nfig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"red\", annotation_text=\"p=0.05 threshold\")\nfig.add_vline(x=0.7, line_dash=\"dash\", line_color=\"blue\", annotation_text=\"High correlation\")\n\nfig.update_layout(\n    width=1000,\n    height=600,\n    template=\"plotly_dark\",\n    xaxis_title='Absolute Correlation (Higher = More Correlated)',\n    yaxis_title='Cointegration P-Value (Lower = More Cointegrated)'\n)\n\nfig.show()\n\n# Identify interesting relationships\nprint(\"\\nüéØ Interesting Relationships:\")\n\n# High correlation but not cointegrated\nhigh_corr_no_coint = comparison_df[(comparison_df['correlation'] > 0.7) & (comparison_df['cointegration_pvalue'] > 0.05)]\nprint(f\"\\nüìà Highly correlated but NOT cointegrated ({len(high_corr_no_coint)} pairs):\")\nif not high_corr_no_coint.empty:\n    for _, row in high_corr_no_coint.head(5).iterrows():\n        print(f\"  ‚Ä¢ {row['pair1']} vs {row['pair2']}: corr={row['correlation']:.3f}, p-val={row['cointegration_pvalue']:.3f}\")\nelse:\n    print(\"  None found\")\n\n# Cointegrated but not highly correlated\ncoint_low_corr = comparison_df[(comparison_df['cointegration_pvalue'] < 0.05) & (comparison_df['correlation'] < 0.5)]\nprint(f\"\\nüìä Cointegrated but LOW correlation ({len(coint_low_corr)} pairs):\")\nif not coint_low_corr.empty:\n    for _, row in coint_low_corr.head(5).iterrows():\n        print(f\"  ‚Ä¢ {row['pair1']} vs {row['pair2']}: corr={row['correlation']:.3f}, p-val={row['cointegration_pvalue']:.3f}\")\nelse:\n    print(\"  None found\")\n\n# Both highly correlated and cointegrated (best for pairs trading)\nboth_strong = comparison_df[(comparison_df['correlation'] > 0.7) & (comparison_df['cointegration_pvalue'] < 0.05)]\nprint(f\"\\n‚≠ê BOTH highly correlated AND cointegrated ({len(both_strong)} pairs - best for pairs trading):\")\nif not both_strong.empty:\n    for _, row in both_strong.head(5).iterrows():\n        print(f\"  ‚Ä¢ {row['pair1']} vs {row['pair2']}: corr={row['correlation']:.3f}, p-val={row['cointegration_pvalue']:.3f}\")\nelse:\n    print(\"  None found\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üìä Hierarchical Clustering and Visualization\n",
    "# Perform clustering analysis and create comprehensive visualizations\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# ============================================\n",
    "# SIMPLIFIED DISTANCE MATRIX TRANSFORMATION\n",
    "# ============================================\n",
    "# Convert cointegration p-values to distance matrix\n",
    "# Use 1 - p_value so that lower p-values (stronger relationships) become higher similarity\n",
    "distance_matrix = 1 - cointegration_matrix.values\n",
    "\n",
    "# Ensure diagonal is 0 (distance from self)\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "# Ensure matrix is symmetric and positive\n",
    "distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
    "distance_matrix = np.maximum(distance_matrix, 0)\n",
    "\n",
    "print(\"‚úÖ Distance matrix transformation complete\")\n",
    "\n",
    "# ============================================\n",
    "# HIERARCHICAL CLUSTERING\n",
    "# ============================================\n",
    "# Convert to condensed form and perform clustering\n",
    "distances_condensed = squareform(distance_matrix, checks=False)\n",
    "Z = linkage(distances_condensed, method='ward')\n",
    "\n",
    "print(\"‚úÖ Hierarchical clustering complete\")\n",
    "\n",
    "# ============================================\n",
    "# PLOTLY DENDROGRAM VISUALIZATION\n",
    "# ============================================\n",
    "# Create interactive dendrogram using plotly\n",
    "# Use the distance matrix directly with plotly's dendrogram function\n",
    "fig = ff.create_dendrogram(\n",
    "    distance_matrix,\n",
    "    orientation='left',\n",
    "    labels=pairs,\n",
    "    linkagefun=lambda x: linkage(x, 'ward')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üìä Hierarchical Clustering Dendrogram - Market Relationships',\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    xaxis_title='Distance',\n",
    "    showlegend=False,\n",
    "    font=dict(size=10),\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ============================================\n",
    "# CLUSTER ASSIGNMENT AND ANALYSIS\n",
    "# ============================================\n",
    "# Define number of clusters and create assignments\n",
    "n_clusters = 5\n",
    "clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "\n",
    "# Create cluster DataFrame\n",
    "cluster_df = pd.DataFrame({\n",
    "    'trading_pair': pairs,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Created {n_clusters} clusters with the following distribution:\")\n",
    "for i in range(1, n_clusters + 1):\n",
    "    cluster_count = (clusters == i).sum()\n",
    "    cluster_members = cluster_df[cluster_df['cluster'] == i]['trading_pair'].tolist()\n",
    "    print(f\"  ‚Ä¢ Cluster {i}: {cluster_count} pairs\")\n",
    "    print(f\"    Members: {', '.join(cluster_members[:5])}{'...' if len(cluster_members) > 5 else ''}\")\n",
    "\n",
    "# ============================================\n",
    "# VOLUME METRICS CALCULATION\n",
    "# ============================================\n",
    "# Calculate volume-based metrics for each trading pair\n",
    "volume_metrics = {}\n",
    "for candle in candles:\n",
    "    if candle.data is not None and not candle.data.empty:\n",
    "        pair_name = candle.trading_pair\n",
    "        # Only include pairs that are in our cluster analysis\n",
    "        if pair_name in pairs:\n",
    "            volumes = candle.data['volume']\n",
    "            avg_volume = volumes.mean()\n",
    "            volume_stability = volumes.std() / avg_volume if avg_volume > 0 else float('inf')\n",
    "            volume_metrics[pair_name] = {\n",
    "                'avg_volume': avg_volume,\n",
    "                'volume_stability': volume_stability\n",
    "            }\n",
    "\n",
    "# Add volume metrics to cluster DataFrame\n",
    "cluster_df['avg_volume'] = cluster_df['trading_pair'].map(\n",
    "    lambda x: volume_metrics.get(x, {}).get('avg_volume', 0)\n",
    ")\n",
    "cluster_df['volume_stability'] = cluster_df['trading_pair'].map(\n",
    "    lambda x: volume_metrics.get(x, {}).get('volume_stability', float('inf'))\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# VOLUME-BASED CLUSTER VISUALIZATION\n",
    "# ============================================\n",
    "# Filter out infinite values for visualization\n",
    "viz_df = cluster_df[cluster_df['volume_stability'] != float('inf')].copy()\n",
    "\n",
    "if not viz_df.empty:\n",
    "    fig = px.scatter(\n",
    "        viz_df,\n",
    "        x='avg_volume',\n",
    "        y='volume_stability',\n",
    "        color='cluster',\n",
    "        hover_data=['trading_pair'],\n",
    "        log_x=True,  # Use logarithmic scale for volume\n",
    "        title='üéØ Market Clusters by Volume Metrics',\n",
    "        labels={\n",
    "            'avg_volume': 'Average Trading Volume (log scale)',\n",
    "            'volume_stability': 'Volume Stability (Std/Mean)',\n",
    "            'cluster': 'Cluster'\n",
    "        },\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        template=\"plotly_dark\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# ============================================\n",
    "# SIMPLIFIED COINTEGRATION HEATMAP\n",
    "# ============================================\n",
    "# Create heatmap showing clusters\n",
    "cluster_matrix = np.zeros((n_pairs, n_pairs))\n",
    "for i in range(n_pairs):\n",
    "    for j in range(n_pairs):\n",
    "        if clusters[i] == clusters[j]:\n",
    "            cluster_matrix[i, j] = clusters[i]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add cointegration heatmap\n",
    "fig.add_trace(go.Heatmap(\n",
    "    z=cointegration_matrix.values,\n",
    "    x=pairs,\n",
    "    y=pairs,\n",
    "    colorscale='RdYlGn_r',\n",
    "    zmin=0,\n",
    "    zmax=0.1,\n",
    "    colorbar=dict(title=\"P-Value\", x=1.05),\n",
    "    name='Cointegration'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üî• Cointegration P-Values Heatmap<br><sub>Green = Strong Cointegration (p < 0.05)</sub>',\n",
    "    width=900,\n",
    "    height=900,\n",
    "    xaxis_tickangle=-45,\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# ============================================\n",
    "# CLUSTER SUMMARY STATISTICS\n",
    "# ============================================\n",
    "print(\"\\nüìä Cluster Cointegration Summary:\")\n",
    "for cluster_num in range(1, n_clusters + 1):\n",
    "    cluster_pairs = cluster_df[cluster_df['cluster'] == cluster_num]['trading_pair'].tolist()\n",
    "    if len(cluster_pairs) > 1:\n",
    "        # Calculate average intra-cluster cointegration\n",
    "        p_values = []\n",
    "        for i, pair1 in enumerate(cluster_pairs):\n",
    "            for pair2 in cluster_pairs[i+1:]:\n",
    "                if pair1 in cointegration_matrix.index and pair2 in cointegration_matrix.columns:\n",
    "                    p_values.append(cointegration_matrix.loc[pair1, pair2])\n",
    "\n",
    "        if p_values:\n",
    "            avg_p = np.mean(p_values)\n",
    "            print(f\"  ‚Ä¢ Cluster {cluster_num}: avg p-value = {avg_p:.4f} ({'Strong' if avg_p < 0.05 else 'Moderate' if avg_p < 0.1 else 'Weak'} cointegration)\")\n",
    "\n",
    "# ============================================\n",
    "# TOP PAIRS SELECTION BY CLUSTER\n",
    "# ============================================\n",
    "def select_top_pairs(cluster_df, n_pairs_per_cluster=2):\n",
    "    \"\"\"Select top trading pairs from each cluster based on volume.\"\"\"\n",
    "    selected_pairs = []\n",
    "    for cluster_num in cluster_df['cluster'].unique():\n",
    "        cluster_pairs = cluster_df[cluster_df['cluster'] == cluster_num].copy()\n",
    "        # Filter out infinite values and select by volume\n",
    "        cluster_pairs = cluster_pairs[cluster_pairs['volume_stability'] != float('inf')]\n",
    "        if not cluster_pairs.empty:\n",
    "            top_pairs = cluster_pairs.nlargest(min(n_pairs_per_cluster, len(cluster_pairs)), 'avg_volume')\n",
    "            selected_pairs.append(top_pairs)\n",
    "\n",
    "    if selected_pairs:\n",
    "        return pd.concat(selected_pairs)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Select and display top pairs\n",
    "top_pairs = select_top_pairs(cluster_df, n_pairs_per_cluster=2)\n",
    "\n",
    "if not top_pairs.empty:\n",
    "    print(\"\\nüèÜ Top Trading Pairs by Cluster (by volume):\")\n",
    "    for cluster_num in sorted(top_pairs['cluster'].unique()):\n",
    "        print(f\"\\nüìä Cluster {cluster_num}:\")\n",
    "        cluster_result = top_pairs[top_pairs['cluster'] == cluster_num]\n",
    "        for _, row in cluster_result.iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['trading_pair']}: vol={row['avg_volume']:,.0f}, stability={row['volume_stability']:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üìã Cluster Data Inspection\n",
    "# Examine specific cluster details and characteristics\n",
    "print(f\"üìä Sample cluster data structure:\")\n",
    "print(f\"  ‚Ä¢ Total pairs in analysis: {len(cluster_df)}\")\n",
    "print(f\"  ‚Ä¢ Columns: {list(cluster_df.columns)}\")\n",
    "print(f\"  ‚Ä¢ Cluster range: {cluster_df['cluster'].min()} to {cluster_df['cluster'].max()}\")\n",
    "\n",
    "# Display sample row for data structure understanding\n",
    "sample_row = cluster_df.iloc[1]\n",
    "print(f\"\\nüìã Sample data point (Row 2):\")\n",
    "for col, val in sample_row.items():\n",
    "    if isinstance(val, float) and not np.isfinite(val):\n",
    "        print(f\"  ‚Ä¢ {col}: {val} (infinite/NaN)\")\n",
    "    elif isinstance(val, float):\n",
    "        print(f\"  ‚Ä¢ {col}: {val:.6f}\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {col}: {val}\")\n",
    "\n",
    "# Show cluster data overview\n",
    "cluster_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# üìà Time Series Cluster Analysis\n",
    "# Visualize price movements and relationships within clusters over time\n",
    "def plot_clusters_timeseries(candles, Z, pairs, cut_height=None, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Create time series visualization of clusters showing price movements.\n",
    "    \n",
    "    Args:\n",
    "        candles: List of candle dataframes\n",
    "        Z: Linkage matrix from hierarchical clustering\n",
    "        pairs: List of trading pair names\n",
    "        cut_height: Height to cut dendrogram (alternative to n_clusters)\n",
    "        n_clusters: Number of clusters to create\n",
    "    \n",
    "    Returns:\n",
    "        clusters: Array of cluster assignments\n",
    "    \"\"\"\n",
    "    # Determine cluster assignments\n",
    "    if cut_height is not None:\n",
    "        clusters = fcluster(Z, cut_height, criterion='distance')\n",
    "    else:\n",
    "        clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    plot_data = []\n",
    "    for candle, pair in zip(candles, pairs):\n",
    "        if candle.data is not None and not candle.data.empty:\n",
    "            df = candle.data.copy()\n",
    "            # Calculate cumulative returns for relative performance comparison\n",
    "            df['cum_returns'] = (1 + df['close'].pct_change().fillna(0)).cumprod()\n",
    "            df['trading_pair'] = pair\n",
    "            # Assign cluster label\n",
    "            pair_cluster = clusters[pairs.index(pair)]\n",
    "            df['cluster'] = f'Cluster {pair_cluster}'\n",
    "            plot_data.append(df)\n",
    "    \n",
    "    # Combine all time series data\n",
    "    combined_df = pd.concat(plot_data, ignore_index=True)\n",
    "    \n",
    "    # Create interactive line plot\n",
    "    fig = px.line(\n",
    "        combined_df,\n",
    "        x='timestamp',\n",
    "        y='cum_returns',\n",
    "        color='cluster',\n",
    "        line_group='trading_pair',\n",
    "        hover_data=['trading_pair', 'close'],\n",
    "        title=f'üìä Cumulative Returns by Cluster ({\"Cut Height: \" + str(cut_height) if cut_height else \"Clusters: \" + str(n_clusters)})',\n",
    "        labels={\n",
    "            'timestamp': 'Time',\n",
    "            'cum_returns': 'Cumulative Returns (1 = baseline)',\n",
    "            'cluster': 'Market Cluster'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        template=\"plotly_dark\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate and display cluster statistics\n",
    "    cluster_stats = combined_df.groupby('cluster').agg({\n",
    "        'cum_returns': ['mean', 'std', 'count'],\n",
    "        'trading_pair': 'nunique'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\nüìä Cluster Performance Statistics:\")\n",
    "    print(cluster_stats)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def plot_clusters_timeseries_normalized(candles, Z, pairs, cut_height=None, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Create normalized price visualization where all pairs start at 1.0.\n",
    "    Better for comparing relative performance across different price levels.\n",
    "    \"\"\"\n",
    "    # Determine cluster assignments\n",
    "    if cut_height is not None:\n",
    "        clusters = fcluster(Z, cut_height, criterion='distance')\n",
    "    else:\n",
    "        clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Prepare normalized data\n",
    "    plot_data = []\n",
    "    for candle, pair in zip(candles, pairs):\n",
    "        if candle.data is not None and not candle.data.empty:\n",
    "            df = candle.data.copy()\n",
    "            # Normalize prices to start at 1.0 for comparison\n",
    "            df['normalized_price'] = df['close'] / df['close'].iloc[0]\n",
    "            df['trading_pair'] = pair\n",
    "            # Assign cluster\n",
    "            pair_cluster = clusters[pairs.index(pair)]\n",
    "            df['cluster'] = f'Cluster {pair_cluster}'\n",
    "            plot_data.append(df)\n",
    "    \n",
    "    # Combine data\n",
    "    combined_df = pd.concat(plot_data, ignore_index=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = px.line(\n",
    "        combined_df,\n",
    "        x='timestamp',\n",
    "        y='normalized_price',\n",
    "        color='cluster',\n",
    "        line_group='trading_pair',\n",
    "        hover_data=['trading_pair', 'close'],\n",
    "        title=f'üéØ Normalized Price Movement by Cluster ({\"Cut Height: \" + str(cut_height) if cut_height else \"Clusters: \" + str(n_clusters)})',\n",
    "        labels={\n",
    "            'timestamp': 'Time',\n",
    "            'normalized_price': 'Normalized Price (1 = start)',\n",
    "            'cluster': 'Market Cluster'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        template=\"plotly_dark\",\n",
    "        showlegend=True,\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "print(\"‚úÖ Time series analysis functions defined\")\n",
    "\n",
    "# Example usage: Normalized price movement visualization\n",
    "print(\"üéØ Generating normalized price movement analysis...\")\n",
    "clusters = plot_clusters_timeseries_normalized(candles, Z, pairs, n_clusters=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# üéØ Advanced Market Selection System\n",
    "# Select representative markets from each cluster using comprehensive metrics\n",
    "def select_representative_markets(candles, Z, pairs, n_clusters, top_n=1):\n",
    "    \"\"\"\n",
    "    Select optimal representative markets from each cluster using multi-factor analysis.\n",
    "    \n",
    "    Combines volume, volatility, and stability metrics to identify the best trading\n",
    "    opportunities within each market segment.\n",
    "    \n",
    "    Args:\n",
    "        candles: List of candle dataframes\n",
    "        Z: Hierarchical clustering linkage matrix\n",
    "        pairs: List of trading pair names\n",
    "        n_clusters: Number of clusters to create\n",
    "        top_n: Number of markets to select per cluster\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Selected markets with comprehensive metrics\n",
    "    \"\"\"\n",
    "    # Get cluster assignments\n",
    "    clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Create base DataFrame with cluster assignments\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'trading_pair': pairs,\n",
    "        'cluster': clusters\n",
    "    })\n",
    "    \n",
    "    print(f\"üîÑ Calculating comprehensive metrics for {len(candles)} markets...\")\n",
    "    \n",
    "    # ============================================\n",
    "    # COMPREHENSIVE METRICS CALCULATION\n",
    "    # ============================================\n",
    "    market_metrics = []\n",
    "    for candle in candles:\n",
    "        if candle.data is not None and not candle.data.empty:\n",
    "            df = candle.data\n",
    "            \n",
    "            # Volume metrics (USD-based)\n",
    "            usd_volume = df['volume'] * df['close']\n",
    "            avg_usd_volume = usd_volume.mean()\n",
    "            volume_stability = usd_volume.std() / usd_volume.mean() if usd_volume.mean() != 0 else float('inf')\n",
    "            \n",
    "            # Price and volatility metrics\n",
    "            returns = df['close'].pct_change().dropna()\n",
    "            volatility = returns.std()\n",
    "            price_mean = df['close'].mean()\n",
    "            \n",
    "            # Trading activity metrics\n",
    "            n_trades = len(df)\n",
    "            price_range = (df['high'].max() - df['low'].min()) / df['close'].mean()\n",
    "            \n",
    "            # Liquidity proxy (high volume + low volatility = good liquidity)\n",
    "            liquidity_score = avg_usd_volume / (volatility + 1e-10)  # Avoid division by zero\n",
    "            \n",
    "            metrics = {\n",
    "                'trading_pair': candle.trading_pair,\n",
    "                'avg_usd_volume': avg_usd_volume,\n",
    "                'volatility': volatility,\n",
    "                'price_mean': price_mean,\n",
    "                'n_trades': n_trades,\n",
    "                'volume_stability': volume_stability,\n",
    "                'price_range': price_range,\n",
    "                'liquidity_score': liquidity_score\n",
    "            }\n",
    "            market_metrics.append(metrics)\n",
    "    \n",
    "    # Create comprehensive metrics DataFrame\n",
    "    metrics_df = pd.DataFrame(market_metrics)\n",
    "    \n",
    "    # Merge with cluster assignments\n",
    "    cluster_df = cluster_df.merge(metrics_df, on='trading_pair', how='left')\n",
    "    \n",
    "    # ============================================\n",
    "    # METRIC NORMALIZATION AND SCORING\n",
    "    # ============================================\n",
    "    # Normalize key metrics to 0-1 scale for fair comparison\n",
    "    for col in ['avg_usd_volume', 'volatility', 'liquidity_score']:\n",
    "        if col in cluster_df.columns:\n",
    "            min_val = cluster_df[col].min()\n",
    "            max_val = cluster_df[col].max()\n",
    "            if max_val > min_val:  # Avoid division by zero\n",
    "                cluster_df[f'{col}_normalized'] = (cluster_df[col] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                cluster_df[f'{col}_normalized'] = 0\n",
    "    \n",
    "    # Calculate composite score with weighted factors\n",
    "    # Higher volume (60%) + Lower volatility (20%) + Higher liquidity (20%)\n",
    "    cluster_df['composite_score'] = (\n",
    "        cluster_df.get('avg_usd_volume_normalized', 0) * 0.6 +      # Volume weight\n",
    "        (1 - cluster_df.get('volatility_normalized', 0)) * 0.2 +    # Inverse volatility weight  \n",
    "        cluster_df.get('liquidity_score_normalized', 0) * 0.2       # Liquidity weight\n",
    "    )\n",
    "    \n",
    "    # ============================================\n",
    "    # MARKET SELECTION BY CLUSTER\n",
    "    # ============================================\n",
    "    selected_markets = []\n",
    "    for cluster_num in range(1, n_clusters + 1):\n",
    "        cluster_markets = cluster_df[cluster_df['cluster'] == cluster_num].copy()\n",
    "        \n",
    "        if not cluster_markets.empty:\n",
    "            # Select top markets based on composite score\n",
    "            top_markets = cluster_markets.nlargest(top_n, 'composite_score')\n",
    "            selected_markets.append(top_markets)\n",
    "    \n",
    "    selected_df = pd.concat(selected_markets) if selected_markets else pd.DataFrame()\n",
    "    \n",
    "    # Sort results by cluster and score\n",
    "    if not selected_df.empty:\n",
    "        selected_df = selected_df.sort_values(['cluster', 'composite_score'], ascending=[True, False])\n",
    "    \n",
    "    # ============================================\n",
    "    # VISUALIZATION\n",
    "    # ============================================\n",
    "    if not cluster_df.empty:\n",
    "        # Create comprehensive scatter plot\n",
    "        fig = px.scatter(\n",
    "            cluster_df,\n",
    "            x='avg_usd_volume',\n",
    "            y='volatility',\n",
    "            color='cluster',\n",
    "            size='liquidity_score',\n",
    "            hover_data=['trading_pair', 'avg_usd_volume', 'volatility', 'volume_stability', 'composite_score'],\n",
    "            title=f'üéØ Market Selection Analysis (Top {top_n} per cluster)',\n",
    "            labels={\n",
    "                'avg_usd_volume': 'Average USD Volume',\n",
    "                'volatility': 'Price Volatility',\n",
    "                'cluster': 'Market Cluster',\n",
    "                'liquidity_score': 'Liquidity Score'\n",
    "            },\n",
    "            log_x=True  # Use log scale for volume\n",
    "        )\n",
    "        \n",
    "        # Highlight selected markets with star markers\n",
    "        if not selected_df.empty:\n",
    "            selected_pairs = selected_df['trading_pair'].tolist()\n",
    "            for pair in selected_pairs:\n",
    "                market_data = cluster_df[cluster_df['trading_pair'] == pair]\n",
    "                if not market_data.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=[market_data['avg_usd_volume'].iloc[0]],\n",
    "                            y=[market_data['volatility'].iloc[0]],\n",
    "                            mode='markers',\n",
    "                            marker=dict(\n",
    "                                symbol='star',\n",
    "                                size=15,\n",
    "                                line=dict(width=2, color='white'),\n",
    "                                color='yellow'\n",
    "                            ),\n",
    "                            name=f'‚≠ê {pair}',\n",
    "                            showlegend=False\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            width=1000,\n",
    "            height=600,\n",
    "            template=\"plotly_dark\",\n",
    "            xaxis_title='üí∞ Average USD Volume (log scale)',\n",
    "            yaxis_title='üìä Price Volatility'\n",
    "        )\n",
    "        fig.show()\n",
    "    \n",
    "    # ============================================\n",
    "    # RESULTS SUMMARY\n",
    "    # ============================================\n",
    "    if not selected_df.empty:\n",
    "        print(f\"\\nüèÜ Selected Representative Markets ({top_n} per cluster):\")\n",
    "        \n",
    "        for cluster_num in range(1, n_clusters + 1):\n",
    "            cluster_results = selected_df[selected_df['cluster'] == cluster_num]\n",
    "            if not cluster_results.empty:\n",
    "                print(f\"\\nüìä Cluster {cluster_num} ({len(cluster_results)} selected):\")\n",
    "                \n",
    "                # Format results for display\n",
    "                display_cols = ['trading_pair', 'avg_usd_volume', 'volatility', 'volume_stability', 'composite_score']\n",
    "                formatted_results = cluster_results[display_cols].copy()\n",
    "                \n",
    "                # Apply formatting\n",
    "                formatted_results['avg_usd_volume'] = formatted_results['avg_usd_volume'].apply(lambda x: f\"${x:,.0f}\")\n",
    "                formatted_results['volatility'] = formatted_results['volatility'].apply(lambda x: f\"{x:.4f}\")\n",
    "                formatted_results['volume_stability'] = formatted_results['volume_stability'].apply(\n",
    "                    lambda x: f\"{x:.4f}\" if np.isfinite(x) else \"‚àû\"\n",
    "                )\n",
    "                formatted_results['composite_score'] = formatted_results['composite_score'].apply(lambda x: f\"{x:.4f}\")\n",
    "                \n",
    "                print(formatted_results.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No markets selected - check data availability\")\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "print(\"üéØ Advanced market selection system ready\")\n",
    "\n",
    "# Execute comprehensive market selection\n",
    "selected_markets = select_representative_markets(\n",
    "    candles=candles,\n",
    "    Z=Z,\n",
    "    pairs=pairs,\n",
    "    n_clusters=3,  # Create 10 distinct market clusters\n",
    "    top_n=2         # Select top 2 markets from each cluster\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Market selection complete: {len(selected_markets)} markets selected from {len(pairs)} total pairs\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quants-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
